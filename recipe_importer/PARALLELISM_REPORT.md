# Rapport : Architecture du parallÃ©lisme â€” Ã©tat des lieux

> Ce rapport documente l'architecture actuelle du parallÃ©lisme dans l'importer.
> Pour le plan d'action de scaling Ã  50 recettes, voir **THROUGHPUT_REPORT.md**.

## Architecture actuelle

```
Importer (main.py)                             Serveur FastAPI
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  asyncio.Queue(urls)                          POST /api/recipes
  N workers (concurrent_imports)                 â””â”€ asyncio.create_task()
  â”‚                                                  â””â”€ subprocess Python
  â”œâ”€ worker 1 â”€â”€â”€ POST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’      (recipe_scraper.cli)
  â”‚               poll (adaptatif 1-5s) â†â”€â”€â”€â”€â†’  GET /progress/{id}
  â”œâ”€ worker 2 â”€â”€â”€ POST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’      ...
  â”‚               poll ...
  â””â”€ worker N â”€â”€â”€ ...
```

Le parallÃ©lisme fonctionne sur 3 niveaux :

| Niveau | MÃ©canisme | LimitÃ© par |
|--------|-----------|------------|
| Client (importer) | `asyncio.Queue` + N workers | `concurrent_imports` (CLI `-c`) |
| Serveur (FastAPI) | `asyncio.create_task` par requÃªte | âš ï¸ Rien actuellement |
| Worker (scraper) | 1 subprocess OS par recette | RAM / CPU machine |

## Ce qui fonctionne bien âœ…

1. **Pattern producer/consumer** â€” les URLs sont dans une `asyncio.Queue`, les workers consomment au fil de l'eau. Pas de crÃ©ation de 500 coroutines d'un coup.

2. **Session aiohttp partagÃ©e** â€” rÃ©utilise les connexions HTTP (keep-alive), bon pour le polling frÃ©quent.

3. **Polling adaptatif** â€” 1s les 30 premiÃ¨res secondes, 3s jusqu'Ã  2 min, puis 5s. RÃ©duit le nombre de requÃªtes de ~60%.

4. **Retry intelligent** â€” ne retente PAS sur les stall timeouts (le subprocess serveur tourne encore). Ne retente QUE sur les erreurs serveur rÃ©elles.

5. **Gestion des doublons** â€” HTTP 409 du serveur â†’ skip propre, pas de retry inutile. L'index URL cÃ´tÃ© serveur rend le check O(1).

6. **`max_stall_s` = 900s** â€” laisse le temps au LLM structuring (3-8 min) sans faux timeout.

## Ce qui pose problÃ¨me âš ï¸

### CÃ´tÃ© serveur : pas de contrÃ´le de concurrence

Le serveur accepte TOUTES les requÃªtes et lance un subprocess immÃ©diatement pour chacune. Avec `-c 50`, Ã§a crÃ©e 50 subprocesses Python (~300 MB chacun) sans aucune protection.

â†’ **Fix dans THROUGHPUT_REPORT.md : sÃ©maphore serveur**

### CÃ´tÃ© serveur : `RecipeService` instanciÃ© par requÃªte

Chaque appel HTTP crÃ©e un nouveau `RecipeService()`, qui reconstruit l'index URL (lecture de tous les fichiers JSON). Aucun Ã©tat partagÃ© entre les requÃªtes.

â†’ **Fix dans THROUGHPUT_REPORT.md : singleton**

### CÃ´tÃ© serveur : dÃ©tection du slug par mtime

`_find_latest_recipe_slug()` utilise `max(mtime)` sur le filesystem â€” race condition avec N subprocesses concurrents.

â†’ **Fix dans THROUGHPUT_REPORT.md : slug via stdout CLI**

## Patterns de 2026 â€” Ã©valuation pragmatique

Le rapport prÃ©cÃ©dent suggÃ©rait Celery, Redis, SSE, etc. Voici une Ã©valuation honnÃªte pour un projet perso :

| Pattern suggÃ©rÃ© | Pertinence | Verdict |
|---|---|---|
| Worker pool (Celery/dramatiq) | Overkill pour 50 recettes | âŒ Un sÃ©maphore asyncio suffit |
| Message broker (Redis) | Ajoute une dÃ©pendance infra | âŒ Pas nÃ©cessaire |
| SSE au lieu du polling | Nice-to-have | ğŸŸ¡ Gain rÃ©el mais pas bloquant |
| Queue bornÃ©e cÃ´tÃ© client | DÃ©jÃ  implÃ©mentÃ© | âœ… `asyncio.Queue` |
| Backpressure serveur (429) | Utile en prod multi-user | ğŸŸ¡ Pas prioritaire |
| Circuit breaker + exp backoff | Utile en rÃ©seau instable | ğŸŸ¡ Local = pas nÃ©cessaire |

**Philosophie :** le bottleneck est le LLM (3-8 min par recette). Optimiser l'infra au-delÃ  d'un sÃ©maphore serveur ne changerait rien au temps total. L'objectif est de **saturer la capacitÃ© machine sans la dÃ©passer**, pas de construire une infra distribuÃ©e.

## MÃ©triques utiles pour le monitoring

Pour suivre les performances en batch de 50 :

```
Throughput     = recettes terminÃ©es / minute
Utilisation    = subprocesses actifs / sÃ©maphore max
Queue depth    = tÃ¢ches en attente de slot serveur
Temps mÃ©dian   = durÃ©e mÃ©diane d'une recette (scrape â†’ save)
Taux de succÃ¨s = succÃ¨s / (succÃ¨s + erreurs)
```

Ces mÃ©triques sont dÃ©jÃ  partiellement trackÃ©es par `ImportMetrics` cÃ´tÃ© client. Le sÃ©maphore serveur permettrait d'exposer `utilisation` et `queue depth` via un endpoint `/api/status` si besoin.
