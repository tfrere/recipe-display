# Rapport : Scaler l'import de recettes √† 50 en batch

## Contexte

**Objectif :** pouvoir importer 50 recettes en un seul batch, de mani√®re fiable et dans un temps raisonnable.

**Architecture actuelle :**
```
Importer (client Python)
  ‚îî‚îÄ N workers asyncio (limit√©s par concurrent_imports)
       ‚îî‚îÄ POST /api/recipes ‚Üí serveur FastAPI
            ‚îî‚îÄ asyncio.create_task ‚Üí subprocess Python (1 par recette)
                 ‚îî‚îÄ recipe_scraper.cli (scrape + LLM structuring + enrichment + save)
       ‚îî‚îÄ GET /api/recipes/progress/{id} (polling adaptatif 1-5s)
```

**Performances actuelles (batch 10, `-c 10`) :**

| M√©trique | Valeur |
|---|---|
| Temps moyen par recette | ~3-8 min (domin√© par le LLM structuring) |
| Workers effectifs | 8-9 sur 10 |
| RAM par subprocess | ~300 MB |
| Taux de succ√®s | ~85-90% |

---

## Ce qui a d√©j√† √©t√© corrig√© ‚úÖ

| Fix | Statut | Impact |
|---|---|---|
| `max_stall_s` pass√© de 300 √† 900s | ‚úÖ Done | -80% de faux timeouts |
| Pas de retry sur stall timeout | ‚úÖ Done | Plus de subprocesses doublons |
| URL index O(1) via `_url_index` | ‚úÖ Done | Lookup instantan√© |
| Pattern producer/consumer avec `asyncio.Queue` | ‚úÖ Done | M√©moire stable c√¥t√© client |
| Polling adaptatif (1s ‚Üí 3s ‚Üí 5s) | ‚úÖ Done | -60% de requ√™tes polling |

---

## üî¥ Les 3 probl√®mes critiques pour scaler √† 50

### 1. `RecipeService()` est recr√©√© √† chaque requ√™te HTTP

```python
# server/api/routes/recipes.py:19-21
def get_recipe_service():
    return RecipeService()  # ‚Üê NOUVELLE instance √† chaque appel !
```

**Cons√©quences avec 50 requ√™tes concurrentes :**

- `_build_url_index()` est appel√© 50 fois en parall√®le ‚Üí chacun lit et parse TOUS les fichiers `.recipe.json` du disque
- Avec 500 recettes existantes : 50 √ó 500 fichiers = **25 000 lectures de fichier JSON** juste pour l'init
- Le dict `generation_tasks` est vide √† chaque fois ‚Üí aucun tracking des t√¢ches en cours
- Le `_url_index` est reconstruit √† chaque POST, mais les recettes ajout√©es par d'autres requ√™tes en cours ne sont pas visibles (isolation totale entre instances)

**Estimation de l'impact :** 2-5s de latence ajout√©e √† CHAQUE requ√™te, plus des race conditions sur les doublons.

**Fix : Singleton `RecipeService`**

```python
# server/api/routes/recipes.py
_recipe_service: RecipeService | None = None

def get_recipe_service():
    global _recipe_service
    if _recipe_service is None:
        _recipe_service = RecipeService()
    return _recipe_service
```

Ou mieux, utiliser une lifespan FastAPI :

```python
# server/main.py
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    app.state.recipe_service = RecipeService()
    yield

app = FastAPI(lifespan=lifespan)
```

### 2. Aucune limite de concurrence c√¥t√© serveur ‚Üí 50 subprocesses = machine morte

Actuellement, `generate_recipe()` appelle `asyncio.create_task()` sans aucune limite :

```python
# server/services/recipe_service.py:922-929
task = asyncio.create_task(
    self._process_recipe_generation(progress_id=progress_id, url=url, ...)
)
```

**Avec 50 requ√™tes simultan√©es :**

| Ressource | 10 concurrent | 50 concurrent |
|---|---|---|
| Subprocesses Python | 10 √ó ~300 MB = 3 GB | 50 √ó ~300 MB = **15 GB** |
| CPU (LLM calls) | G√©rable | Thrashing, context switching |
| Disk I/O | OK | Contention sur l'√©criture des .recipe.json |

La machine (ou le container Railway) n'a probablement pas 15 GB de RAM libre. R√©sultat : OOM kills, swap, lenteur extr√™me.

**Fix : S√©maphore serveur pour limiter les subprocesses actifs**

```python
# server/services/recipe_service.py
class RecipeService:
    # Limiter √† N subprocesses simultan√©s (adapt√© √† la RAM dispo)
    _subprocess_semaphore = asyncio.Semaphore(8)

    async def _process_recipe_generation(self, progress_id, url, ...):
        # Les √©tapes l√©g√®res (check_existence) restent hors s√©maphore
        await self._check_existence(progress_id, url)
        
        # Le subprocess lourd attend un slot
        async with self._subprocess_semaphore:
            await self._run_scraper_subprocess(progress_id, url, ...)
```

Le client peut envoyer 50 requ√™tes d'un coup. Le serveur les accepte toutes (r√©ponse imm√©diate avec `progressId`) mais n'ex√©cute que 8 subprocesses √† la fois. Les autres attendent dans la queue asyncio, et le client les voit en status `in_progress` avec le step `check_existence` compl√©t√©.

**Impact :** RAM contr√¥l√©e (~2.5 GB max), throughput maximal sans surcharger la machine.

### 3. `_find_latest_recipe_slug()` est une race condition avec N concurrent

```python
# server/services/recipe_service.py:381-389
def _find_latest_recipe_slug(self) -> Optional[str]:
    recipe_files = list(self.recipes_path.glob("*.recipe.json"))
    if not recipe_files:
        return None
    latest_file = max(recipe_files, key=lambda p: p.stat().st_mtime)
    slug = latest_file.stem.replace(".recipe", "")
    return slug
```

Cette m√©thode retourne le fichier `.recipe.json` le plus r√©cent sur le disque. Avec 50 subprocesses qui terminent √† quelques secondes d'intervalle :

- Subprocess A finit et sauve `poulet-roti.recipe.json` √† 12:00:01
- Subprocess B finit et sauve `tarte-pommes.recipe.json` √† 12:00:02
- Le handler de A appelle `_find_latest_recipe_slug()` ‚Üí retourne `tarte-pommes` (le fichier de B !) au lieu de `poulet-roti`
- Le progress de A est marqu√© "completed" avec le mauvais slug

**Fix : Le CLI doit retourner le slug dans sa sortie stdout**

Faire en sorte que `recipe_scraper.cli` imprime une ligne structur√©e quand il sauvegarde :

```
>>> Saved recipe: slug=poulet-roti
```

Puis dans `_run_cli_and_stream_logs`, parser cette ligne pour extraire le slug :

```python
if ">>> Saved recipe: slug=" in line_text:
    saved_slug = line_text.split("slug=")[1].strip()
```

Cela √©limine la race condition et le scan du filesystem.

---

## üü† Probl√®mes secondaires (optimisation)

### 4. Polling HTTP : 50 concurrent √ó toutes les 3s = ~17 req/s juste pour le suivi

Pas bloquant pour 50 recettes, mais √ßa fait ~1000 req/min de polling pur. FastAPI g√®re √ßa facilement, mais c'est du gaspillage.

**Fix optionnel : Polling plus espac√© pour les t√¢ches "en queue"**

Si le s√©maphore serveur fait attendre une t√¢che, le status reste `in_progress` sur `check_existence` sans changement. On peut d√©tecter √ßa c√¥t√© client et espacer le polling √† 10-15s pour les t√¢ches en attente :

```python
# recipe_processors.py ‚Äî dans _poll_until_done
if current_step == "check_existence" and elapsed > 60:
    await asyncio.sleep(10)  # La t√¢che est probablement en queue serveur
```

### 5. Stats partag√©es via `dict` mutable

```python
stats["in_progress"] += 1  # Pas de lock
```

En asyncio single-thread c'est safe car il n'y a pas de vrai parall√©lisme. Mais si un jour on passe en multiprocessing, √ßa casse. Pour 50 workers, on pourrait utiliser un `asyncio.Lock` l√©ger :

```python
async with stats_lock:
    stats["in_progress"] += 1
```

**Verdict :** pas prioritaire, asyncio single-thread prot√®ge naturellement.

---

## Plan d'impl√©mentation

### Phase 1 : Fixes critiques (30 min de travail)

| # | Fix | Fichier | Effort |
|---|---|---|---|
| 1 | Singleton `RecipeService` | `server/api/routes/recipes.py` | 5 min |
| 2 | S√©maphore serveur (8 subprocess max) | `server/services/recipe_service.py` | 15 min |
| 3 | Slug retourn√© par le CLI stdout | `recipe_scraper/cli.py` + `recipe_service.py` | 10 min |

**Apr√®s Phase 1 :** on peut lancer `-c 50` en s√©curit√©. Le serveur throttle √† 8 subprocesses actifs, la RAM reste sous contr√¥le, et les slugs sont correctement attribu√©s.

### Phase 2 : Optimisation du client (15 min)

| # | Fix | Fichier | Effort |
|---|---|---|---|
| 4 | Polling espac√© pour les t√¢ches en queue | `recipe_processors.py` | 10 min |
| 5 | Logging am√©lior√© pour les batches (ETA, throughput) | `progress_tracker.py` | 5 min |

### Phase 3 : Nice-to-have (si besoin)

| # | Fix | Impact | Effort |
|---|---|---|---|
| 6 | SSE au lieu du polling | -95% de requ√™tes HTTP polling | 2-3h |
| 7 | Endpoint `DELETE /progress/{id}` pour cancel | Nettoyage propre des subprocesses | 1h |
| 8 | Backpressure HTTP 429 si queue pleine | Protection contre les abus | 30 min |

---

## Projection des performances

### Batch de 50 recettes avec les fixes Phase 1 + 2

```
Config : -c 50, s√©maphore serveur = 8

Timeline estim√©e :
  [0-10s]    50 POST envoy√©s, 50 progressId re√ßus
  [0-10s]    8 subprocesses lanc√©s, 42 en queue serveur
  [3-8 min]  Les 8 premiers terminent ‚Üí 8 nouveaux partent
  [~35 min]  50/50 termin√©s (50 recettes √∑ 8 slots √ó ~5 min/recette)

M√©triques attendues :
  Temps total : ~30-40 min (vs ~25 min th√©orique avec 10 subprocesses non-limit√©s)
  RAM serveur : ~2.5 GB stable (vs 15 GB sans s√©maphore)
  Taux de succ√®s : ~90%+ (pas de race conditions slug)
  Polling requests : ~3000 total (50 √ó ~60 polls √ó 3s interval moyen)
```

### Comparatif

| Sc√©nario | Temps total | RAM max | Fiabilit√© |
|---|---|---|---|
| Actuel (`-c 10`, pas de s√©maphore) | ~50 min | ~3 GB | ~85% |
| Phase 1 (`-c 50`, s√©maphore 8) | ~35 min | ~2.5 GB | ~90% |
| Phase 1+2 (`-c 50`, polling smart) | ~35 min | ~2.5 GB | ~92% |
| Th√©orique max (s√©maphore 12, 16GB RAM) | ~25 min | ~4 GB | ~90% |

---

## R√©sum√© : ce qui bloque le scale √† 50

| Cause | S√©v√©rit√© | Impact | Fix |
|---|---|---|---|
| `RecipeService()` recr√©√© par requ√™te | üî¥ Critique | 25K lectures fichier inutiles, no state sharing | Singleton |
| Pas de limite subprocess serveur | üî¥ Critique | 15 GB RAM ‚Üí OOM | S√©maphore asyncio |
| `_find_latest_recipe_slug()` race condition | üî¥ Critique | Mauvais slug attribu√© | Slug via stdout CLI |
| Polling trop fr√©quent pour t√¢ches en queue | üü† Medium | ~1K req/min inutiles | Polling adaptatif |
| Stats sans lock | üü° Low | Safe en asyncio, fragile si migration | Asyncio Lock |

**En appliquant les 3 fixes critiques**, on peut passer de `-c 10` √† `-c 50` en toute s√©curit√©. Le bottleneck devient alors le temps LLM par recette (~3-8 min), qui est incompressible. Le s√©maphore serveur garantit qu'on utilise 100% de la capacit√© machine sans la d√©passer.
